{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision APIs\n",
    "\n",
    "There are some APIs which can support image analysis on GCP. \n",
    "\n",
    "- Cloud Vision API\n",
    "- Gemini Pro Vision\n",
    "- Imagen 1 / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "bucket_name = os.environ.get(\"BUCKET_UPLOAD_TEMP\")\n",
    "\n",
    "def upload_file_to_temp_bucket(file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.upload_from_filename(file_name)\n",
    "    \n",
    "    return blob.public_url.replace(\"https://storage.googleapis.com/\", \"gs://\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "file_uri = upload_file_to_temp_bucket(\"resources/wooribank_login.png\")\n",
    "\n",
    "image = vision.Image()\n",
    "image.source.image_uri = file_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_object_detection():\n",
    "  objects = client.object_localization(image=image).localized_object_annotations\n",
    "  print(f\"Number of objects found: {len(objects)}\")\n",
    "  for object_ in objects:\n",
    "    print(f\"\\n{object_.name} (confidence: {object_.score})\")\n",
    "    print(\"Normalized bounding polygon vertices: \")\n",
    "    for vertex in object_.bounding_poly.normalized_vertices:\n",
    "      print(f\" - ({vertex.x}, {vertex.y})\")\n",
    "\n",
    "test_object_detection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - label detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_label_detection():\n",
    "  response = client.label_detection(image=image)\n",
    "  labels = response.label_annotations\n",
    "  print(\"Labels:\")\n",
    "\n",
    "  for label in labels:\n",
    "    print(label)\n",
    "\n",
    "  if response.error.message:\n",
    "    raise Exception(\n",
    "      \"{}\\nFor more info on error messages, check: \"\n",
    "      \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "    )\n",
    "\n",
    "test_label_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Pro Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - Image analysis with prompt 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_multimodal_model():\n",
    "    from vertexai.preview.generative_models import (\n",
    "        GenerationConfig,\n",
    "        GenerativeModel,\n",
    "        Image,\n",
    "        Part,\n",
    "        HarmBlockThreshold,\n",
    "        HarmCategory,\n",
    "    )\n",
    "    multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "    return multimodal_model\n",
    "\n",
    "multimodal_model = get_multimodal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multimodal_generation():\n",
    "    from vertexai.preview.generative_models import (\n",
    "        GenerationConfig,\n",
    "        GenerativeModel,\n",
    "        Image,\n",
    "        Part,\n",
    "        HarmBlockThreshold,\n",
    "        HarmCategory,\n",
    "    )\n",
    "    prompt = \"Please provide me with the input fields and their locations in the provided screenshot.\"\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=2048,\n",
    "    )\n",
    "\n",
    "    safety_config = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "\n",
    "    prompt = \"Please provide me with the input fields and their locations in the provided screenshot.\"\n",
    "\n",
    "    image = Part.from_uri(\n",
    "        uri=file_uri,\n",
    "        mime_type=\"image/png\",\n",
    "    )\n",
    "    contents = [prompt, image]\n",
    "\n",
    "    responses = multimodal_model.generate_content(contents, generation_config=generation_config, \n",
    "        safety_settings=safety_config, stream=False)\n",
    "\n",
    "    for response in responses:\n",
    "        print(response)\n",
    "\n",
    "test_multimodal_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen 1 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.vision_models import ImageTextModel\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = ImageTextModel.from_pretrained(\"imagetext@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeskey\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_ask_question_with_imagen():\n",
    "    from vertexai.vision_models import Image\n",
    "\n",
    "    source_image = Image.load_from_file(location='resources/wooribank_login.png')\n",
    "\n",
    "    prompt = \"Please provide me with the input fields and their locations in the provided screenshot.\"\n",
    "\n",
    "    answers = model.ask_question(\n",
    "        image=source_image,\n",
    "        question=prompt,\n",
    "    )\n",
    "\n",
    "    for answer in answers:\n",
    "        print(answer)\n",
    "\n",
    "test_ask_question_with_imagen()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
